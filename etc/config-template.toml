# TiKV config template
#  Human-readable big numbers: 
#   File size(based on byte): KB, MB, GB, TB, PB (or lowercase)
#    e.g.: 1_048_576 = "1MB"
#   Time(based on ms): ms, s, m, h
#    e.g.: 78_000 = "1.3m"

[server]
# set listening address.
addr = "127.0.0.1:20160"
# set advertise listening address for client communication, if not set, use addr instead.
#advertise-addr = ""
# set the path to rocksdb directory.
store = "/tmp/tikv/store"
# set attributes about this server, e.g. "zone=us-west-1,disk=ssd".
labels = ""
# log level: trace, debug, info, warn, error, off.
log-level = "info"
# notify capacity, 40960 is suitable for about 7000 regions.
notify-capacity = 40960
# maximum number of messages can be processed in one tick.
messages-per-tick = 4096
# socket send/recv buffer size.
send-buffer-size = "128KB"
recv-buffer-size = "128KB"
# size of thread pool for endpoint task
end-point-concurrency = 8

# set store capacity, if no set, use disk capacity.
# capacity = 0

# set backup path, if not set, use "backup" under store path.
# backup = "/tmp/tikv/store/backup"

[metric]
# the Prometheus client push interval. Setting the value to 0s stops Prometheus client from pushing.
interval = "15s"
# the Prometheus pushgateway address. Leaving it empty stops Prometheus client from pushing.
address = ""
# the Prometheus client push job name. Note: A node id will automatically append, e.g., "tikv_1".
job = "tikv"

[raftstore]
# notify capacity, 40960 is suitable for about 7000 regions.
notify-capacity = 40960

# maximum number of messages can be processed in one tick.
messages-per-tick = 4096

# Region heartbeat tick interval for reporting to pd. 
pd-heartbeat-tick-interval = "60s"
# Store heartbeat tick interval for reporting to pd.
pd-store-heartbeat-tick-interval = "10s"

# When the region's size exceeds region-max-size, we will split the region 
# into two which the left region's size will be region-split-size or a little 
# bit smaller. 
region-max-size = "144MB"
region-split-size = "96MB"
# When region size changes exceeds region-split-check-diff, we should check 
# whether the region should be split or not. 
region-split-check-diff = "12MB"
# Interval to check region whether need to be split or not.
split-region-check-tick-interval = "5s"

# When raft entry exceed the max size, reject to propose the entry.
# raft-entry-max-size = "8MB"

# Interval to gc unnecessary raft log.
# raft-log-gc-tick-interval = "10s"
# A threshold to gc stale raft log, must >= 1.
# raft-log-gc-threshold = 50
# When entry count exceed this value, gc will be forced trigger.
# raft-log-gc-count-limit = 48000
# When the approximate size of raft log entries exceed this value, gc will be forced trigger.
# It's recommanded to set it to 3/4 of region-split-size.
# raft-log-gc-size-limit = "48MB"

# When a peer hasn't been active for max-peer-down-duration,
# we will consider this peer to be down and report it to pd.
max-peer-down-duration = "5m"

# Interval to check whether start manual compaction for a region,
# 0 means disable manual compaction.
# region-compact-check-interval = "5m"
# When delete keys of a region exceeds the size, a compaction will be started.
# region-compact-delete-keys-count = 1000000
# Interval to check whether start lock compaction for a region.
# lock-cf-compact-interval = "10m"

# Interval (s) to check region whether the data are consistent.
# consistency-check-interval = 0

[pd]
# pd endpoints 
endpoints = ""

[rocksdb]
# Maximum number of concurrent background compaction jobs, submitted to
# the default LOW priority thread pool.
max-background-compactions = 6

# Maximum number of concurrent background flush jobs
max-background-flushes = 2

# Number of open files that can be used by the DB.  You may need to
# increase this if your database has a large working set. Value -1 means
# files opened are always kept open. You can estimate number of files based
# on target_file_size_base and target_file_size_multiplier for level-based
# compaction.
# If max-open-files = -1, RocksDB will prefetch index and filter blocks into
# block cache at startup, so if your database has a large working set, it will
# take several minutes to open the db.
max-open-files = 40960

# Max size of rocksdb's MANIFEST file.
# For detailed explanation please refer to https://github.com/facebook/rocksdb/wiki/MANIFEST
max-manifest-file-size = "20MB"

# If true, the database will be created if it is missing.
create-if-missing = true

# rocksdb wal recovery mode
# 0 : TolerateCorruptedTailRecords, tolerate incomplete record in trailing data on all logs;
# 1 : AbsoluteConsistency, We don't expect to find any corruption in the WAL;
# 2 : PointInTimeRecovery, Recover to point-in-time consistency;
# 3 : SkipAnyCorruptedRecords, Recovery after a disaster;
wal-recovery-mode = 2

# rocksdb max total wal size
# max-total-wal-size = "4GB"

# Rocksdb Statistics provides cumulative stats over time.
# Set it ture only when debugging performance, because it will introduce overhead.
enable-statistics = false

# Dump statistics periodically in information logs.
# Same as rocksdb's default value (10 min).
stats-dump-period-sec = 600

# If true, then the contents of manifest and data files are not synced
# to stable storage. Their contents remain in the OS buffers till the
# OS decides to flush them. This option is good for bulk-loading
# of data. Once the bulk-loading is complete, please issue a
# sync to the OS to flush all dirty buffers to stable storage.
# Default false.
# disable-data-sync = false

# Due to Rocksdb FAQ: https://github.com/facebook/rocksdb/wiki/RocksDB-FAQ,
# If you want to use rocksdb on multi disks or spinning disks, you should set value at
# least 2MB;
# compaction_readahead_size = 0

# Column Family default used to store actual data of the database.
[rocksdb.defaultcf]
# compression method (if any) is used to compress a block.
#   no:     kNoCompression
#   snappy: kSnappyCompression
#   zlib:   kZlibCompression
#   bzip2:  kBZip2Compression
#   lz4:    kLZ4Compression
#   lz4hc:  kLZ4HCCompression

# per level compression
compression-per-level = "lz4:lz4:lz4:lz4:lz4:lz4:lz4"

# Approximate size of user data packed per block.  Note that the
# block size specified here corresponds to uncompressed data.
block-size = "64KB"

# If you're doing point lookups you definitely want to turn bloom filters on, We use
# bloom filters to avoid unnecessary disk reads. Default bits_per_key is 10, which
# yields ~1% false positive rate. Larger bits_per_key values will reduce false positive
# rate, but increase memory usage and space amplification.
bloom-filter-bits-per-key = 10

# false means one sst file one bloom filter, true means evry block has a corresponding bloom filter
block-based-bloom-filter = false

# Soft limit on number of level-0 files. We start slowing down writes at this point.
level0-slowdown-writes-trigger = 12

# Maximum number of level-0 files.  We stop writes at this point.
level0-stop-writes-trigger = 16

# Amount of data to build up in memory (backed by an unsorted log
# on disk) before converting to a sorted on-disk file.
write-buffer-size = "128MB"

# The maximum number of write buffers that are built up in memory.
max-write-buffer-number = 5

# The minimum number of write buffers that will be merged together
# before writing to storage.
min-write-buffer-number-to-merge = 1

# Control maximum total data size for base level (level 1).
max-bytes-for-level-base = "128MB"

# Target file size for compaction.
target-file-size-base = "32MB"

# block-cache used to cache uncompressed blocks, big block-cache can speed up read
block-cache-size = "1GB"

# Indicating if we'd put index/filter blocks to the block cache.
# If not specified, each "table reader" object will pre-load index/filter block
# during table initialization.
# cache-index-and-filter-blocks = true

# options for Column Family write
# Column Family write used to store commit informations in MVCC model
[rocksdb.writecf]
compression-per-level = "lz4:lz4:lz4:lz4:lz4:lz4:lz4"
block-size = "64KB"
write-buffer-size = "128MB"
max-write-buffer-number = 5
min-write-buffer-number-to-merge = 1
max-bytes-for-level-base = "128MB"
target-file-size-base = "32MB"
block-cache-size = "256MB"
# cache-index-and-filter-blocks = true

# options for Column Family raft.
# Column Family raft is used to store raft log and raft states.
[rocksdb.raftcf]
compression-per-level = "lz4:lz4:lz4:lz4:lz4:lz4:lz4"
block-size = "64KB"
write-buffer-size = "128MB"
max-write-buffer-number = 5
min-write-buffer-number-to-merge = 1
max-bytes-for-level-base = "128MB"
target-file-size-base = "32MB"
block-cache-size = "256MB"

[storage]
# notify capacity of scheduler's channel
scheduler-notify-capacity = 10240

# maximum number of messages can be processed in one tick
scheduler-messages-per-tick = 1024

scheduler-concurrency = 1024000

# scheduler's worker pool size
scheduler-worker-pool-size = 4
