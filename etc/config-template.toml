# TiKV config template
#  Human-readable big numbers:
#   File size(based on byte): KB, MB, GB, TB, PB (or lowercase)
#    e.g.: 1_048_576 = "1MB"
#   Time(based on ms): ms, s, m, h
#    e.g.: 78_000 = "1.3m"

[server]
# set listening address.
addr = "127.0.0.1:20160"
# set advertise listening address for client communication, if not set, use addr instead.
#advertise-addr = ""
# set the path to rocksdb directory.
data-dir = "/tmp/tikv/store"
# set attributes about this server, e.g. "zone=us-west-1,disk=ssd".
labels = ""
# log level: trace, debug, info, warn, error, off.
log-level = "info"
# notify capacity, 40960 is suitable for about 7000 regions.
notify-capacity = 40960
# maximum number of messages can be processed in one tick.
messages-per-tick = 4096
# socket send/recv buffer size.
send-buffer-size = "128KB"
recv-buffer-size = "128KB"
# size of thread pool for endpoint task, should less than total cpu cores.
# end-point-concurrency = 8

# set store capacity, if no set, use disk capacity.
# capacity = 0

# set backup path, if not set, use "backup" under store path.
# backup-dir = "/tmp/tikv/store/backup"

[metric]
# the Prometheus client push interval. Setting the value to 0s stops Prometheus client from pushing.
interval = "15s"
# the Prometheus pushgateway address. Leaving it empty stops Prometheus client from pushing.
address = ""
# the Prometheus client push job name. Note: A node id will automatically append, e.g., "tikv_1".
job = "tikv"

[raftstore]
# notify capacity, 40960 is suitable for about 7000 regions.
notify-capacity = 40960

# maximum number of messages can be processed in one tick.
messages-per-tick = 4096

# Region heartbeat tick interval for reporting to pd.
pd-heartbeat-tick-interval = "60s"
# Store heartbeat tick interval for reporting to pd.
pd-store-heartbeat-tick-interval = "10s"

# When the region's size exceeds region-max-size, we will split the region
# into two which the left region's size will be region-split-size or a little
# bit smaller.
region-max-size = "80MB"
region-split-size = "64MB"
# When region size changes exceeds region-split-check-diff, we should check
# whether the region should be split or not.
region-split-check-diff = "8MB"
# Interval to check region whether need to be split or not.
split-region-check-tick-interval = "5s"

# When raft entry exceed the max size, reject to propose the entry.
# raft-entry-max-size = "8MB"

# Interval to gc unnecessary raft log.
# raft-log-gc-tick-interval = "10s"
# A threshold to gc stale raft log, must >= 1.
# raft-log-gc-threshold = 50
# When entry count exceed this value, gc will be forced trigger.
# raft-log-gc-count-limit = 48000
# When the approximate size of raft log entries exceed this value, gc will be forced trigger.
# It's recommanded to set it to 3/4 of region-split-size.
# raft-log-gc-size-limit = "48MB"

# When a peer hasn't been active for max-peer-down-duration,
# we will consider this peer to be down and report it to pd.
max-peer-down-duration = "5m"

# Interval to check whether start manual compaction for a region,
# 0 is the default value, means disable manual compaction.
# region-compact-check-interval = "5m"
# When delete keys of a region exceeds the size, a compaction will be started.
# region-compact-delete-keys-count = 1000000
# Interval to check whether should start a manual compaction for lock column family,
# if written bytes reach lock-cf-compact-threshold for lock column family, will fire
# a manual compaction for lock column family.
# lock-cf-compact-interval = "10m"
# lock-cf-compact-threshold = "256MB"

# Interval (s) to check region whether the data are consistent.
# consistency-check-interval = 0

[pd]
# pd endpoints
endpoints = ""

[rocksdb]
# RocksDB info log directory, if set, RocksDB will save the log in `info-log-dir/LOG`.
# info-log-dir = ""
# Maximal size of the info log, if new info log will be created if size is large than it.
# info-log-max-size = 0
# Time for the info log file to roll (in seconds).
# info-log-roll-time = "0s"

# Maximum number of concurrent background compaction jobs, submitted to
# the default LOW priority thread pool.
max-background-compactions = 6

# Maximum number of concurrent background flush jobs
max-background-flushes = 2

# Suggested number of concurrent background compaction jobs.
# base-background-compactions = 1

# This value represents the maximum number of threads that will concurrently perform a
# compaction job by breaking it into multiple, smaller ones that are run simultaneously.
# Default: 1 (i.e. no subcompactions)
# max-sub-compactions = 1

# Number of open files that can be used by the DB.  You may need to
# increase this if your database has a large working set. Value -1 means
# files opened are always kept open. You can estimate number of files based
# on target_file_size_base and target_file_size_multiplier for level-based
# compaction.
# If max-open-files = -1, RocksDB will prefetch index and filter blocks into
# block cache at startup, so if your database has a large working set, it will
# take several minutes to open the db.
# max-open-files = 40960

# Max size of rocksdb's MANIFEST file.
# For detailed explanation please refer to https://github.com/facebook/rocksdb/wiki/MANIFEST
max-manifest-file-size = "20MB"

# If true, the database will be created if it is missing.
create-if-missing = true

# rocksdb wal recovery mode
# 0 : TolerateCorruptedTailRecords, tolerate incomplete record in trailing data on all logs;
# 1 : AbsoluteConsistency, We don't expect to find any corruption in the WAL;
# 2 : PointInTimeRecovery, Recover to point-in-time consistency;
# 3 : SkipAnyCorruptedRecords, Recovery after a disaster;
wal-recovery-mode = 2

# rocksdb write-ahead logs dir path
# This specifies the absolute dir path for write-ahead logs (WAL).
# If it is empty, the log files will be in the same dir as data.
# When you set the path to rocksdb directory in memory like in /dev/shm, you may want to set
# wal-dir to a directory on a persistent storage.
# See https://github.com/facebook/rocksdb/wiki/How-to-persist-in-memory-RocksDB-database
# wal-dir = "/tmp/tikv/store"

# The following two fields affect how archived write-ahead logs will be deleted.
# 1. If both set to 0, logs will be deleted asap and will not get into the archive.
# 2. If wal-ttl-seconds is 0 and wal-size-limit is not 0,
#    WAL files will be checked every 10 min and if total size is greater
#    then wal-size-limit, they will be deleted starting with the
#    earliest until size_limit is met. All empty files will be deleted.
# 3. If wal-ttl-seconds is not 0 and wal-size-limit is 0, then
#    WAL files will be checked every wal-ttl-seconds / 2 and those that
#    are older than wal-ttl-seconds will be deleted.
# 4. If both are not 0, WAL files will be checked every 10 min and both
#    checks will be performed with ttl being first.
# When you set the path to rocksdb directory in memory like in /dev/shm, you may want to set
# wal-ttl-seconds to a value greater than 0 (like 86400) and backup your db on a regular basis.
# See https://github.com/facebook/rocksdb/wiki/How-to-persist-in-memory-RocksDB-database
# wal-ttl-seconds = 0
# wal-size-limit = 0

# rocksdb max total wal size
# max-total-wal-size = "4GB"

# rocksdb writable file max buffer size
# writable-file-max-buffer-size = "1MB"

# Rocksdb Statistics provides cumulative stats over time.
# Turn statistics on will introduce about 5%-10% overhead for RocksDB,
# but it is worthy to know the internal status of RocksDB.
# enable-statistics = true

# Dump statistics periodically in information logs.
# Same as rocksdb's default value (10 min).
stats-dump-period-sec = 600

# Due to Rocksdb FAQ: https://github.com/facebook/rocksdb/wiki/RocksDB-FAQ,
# If you want to use rocksdb on multi disks or spinning disks, you should set value at
# least 2MB;
# compaction-readahead-size = 0

# Limit the disk IO of compaction and flush.
# rate-bytes-per-sec = 0

# Column Family default used to store actual data of the database.
[rocksdb.defaultcf]
# compression method (if any) is used to compress a block.
#   no:     kNoCompression
#   snappy: kSnappyCompression
#   zlib:   kZlibCompression
#   bzip2:  kBZip2Compression
#   lz4:    kLZ4Compression
#   lz4hc:  kLZ4HCCompression
#   zstd:   kZSTD

# per level compression
compression-per-level = "lz4:lz4:lz4:lz4:lz4:lz4:lz4"

# Approximate size of user data packed per block.  Note that the
# block size specified here corresponds to uncompressed data.
block-size = "64KB"

# If you're doing point lookups you definitely want to turn bloom filters on, We use
# bloom filters to avoid unnecessary disk reads. Default bits_per_key is 10, which
# yields ~1% false positive rate. Larger bits_per_key values will reduce false positive
# rate, but increase memory usage and space amplification.
bloom-filter-bits-per-key = 10

# false means one sst file one bloom filter, true means evry block has a corresponding bloom filter
block-based-bloom-filter = false

# Soft limit on number of level-0 files. We start slowing down writes at this point.
#level0-slowdown-writes-trigger = 20

# Maximum number of level-0 files.  We stop writes at this point.
#level0-stop-writes-trigger = 36

# Amount of data to build up in memory (backed by an unsorted log
# on disk) before converting to a sorted on-disk file.
write-buffer-size = "128MB"

# The maximum number of write buffers that are built up in memory.
max-write-buffer-number = 5

# The minimum number of write buffers that will be merged together
# before writing to storage.
min-write-buffer-number-to-merge = 1

# Control maximum total data size for base level (level 1).
max-bytes-for-level-base = "128MB"

# Target file size for compaction.
target-file-size-base = "32MB"

# block-cache used to cache uncompressed blocks, big block-cache can speed up read.
# in normal cases should tune to 30%-50% system's total memory.
# block-cache-size = "1GB"

# Indicating if we'd put index/filter blocks to the block cache.
# If not specified, each "table reader" object will pre-load index/filter block
# during table initialization.
# cache-index-and-filter-blocks = true

# options for Column Family write
# Column Family write used to store commit informations in MVCC model
[rocksdb.writecf]
compression-per-level = "lz4:lz4:lz4:lz4:lz4:lz4:lz4"
block-size = "64KB"
write-buffer-size = "128MB"
max-write-buffer-number = 5
min-write-buffer-number-to-merge = 1
max-bytes-for-level-base = "128MB"
target-file-size-base = "32MB"

# in normal cases should tune to 10%-30% system's total memory.
# block-cache-size = "256MB"

# cache-index-and-filter-blocks = true

# options for Column Family raft.
# Column Family raft is used to store raft log and raft states.
[rocksdb.raftcf]
compression-per-level = "lz4:lz4:lz4:lz4:lz4:lz4:lz4"
block-size = "64KB"
write-buffer-size = "128MB"
max-write-buffer-number = 5
min-write-buffer-number-to-merge = 1
max-bytes-for-level-base = "128MB"
target-file-size-base = "32MB"
# should tune to 256MB~2GB.
# block-cache-size = "256MB"

[storage]
# notify capacity of scheduler's channel
# scheduler-notify-capacity = 10240

# maximum number of messages can be processed in one tick
# scheduler-messages-per-tick = 1024

# the number of slots in scheduler latches, concurrency control for write.
# scheduler-concurrency = 102400

# scheduler's worker pool size, should increase it in heavy write cases,
# also should less than total cpu cores.
# scheduler-worker-pool-size = 4
